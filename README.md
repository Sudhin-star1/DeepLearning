# DeepLearning

# ğŸ§  AI Engineering Portfolio â€” Core Modeling & Optimization Tasks

This repository contains foundational projects implemented from scratch in **Kaggle Notebooks**, designed to demonstrate deep understanding and practical application of:

- Linear Models & Bayesian Inference  
- Dimensionality Reduction (PCA)  
- Optimization Techniques  
- Neural Network Training (Backpropagation)  
- Algorithmic Fairness & Model Calibration  
- Advanced Optimizers (SGD, Adam, Adagrad)

These tasks were completed as part of a portfolio required for demonstrating hands-on AI engineering skills.

---

## ğŸ—‚ï¸ Project Structure

| Task | Description |
|------|-------------|
| `Deep Learning Framework and Tools/` | Train linear classifiers using scikit-learn, assess performance |
| `Statistical Predictive Modeling/` | Compare Maximum Likelihood vs Marginal Likelihood (Bayesian Linear Regression) |
| `Mathematics for Deep Learning/` | From-scratch PCA, gradient descent on 4th-order polynomial, and full backpropagation pipeline |
| `Deep Learning Under the Hood/` | Optimization algorithms (SGD, Adam, Adagrad), plus addressing fairness, calibration, and shortcut learning |

---

## âœ… Highlights of Each Task

### ğŸ“Š Linear Predictive Models
- Fit linear models using `LogisticRegression` and `LinearSVC`
- Evaluate accuracy and interpret coefficients
- Use cross-validation to assess generalization

### ğŸ“ˆ MLE vs Marginal Likelihood
- Implement Bayesian Linear Regression
- Estimate predictive distribution via sampling
- Compare parameter uncertainty and model expressiveness
- Visualize uncertainty via confidence intervals

### ğŸ§® PCA from Scratch
- Apply PCA to real-world Iris dataset
- Eigen decomposition without using sklearn's PCA
- Visualize top-2 component projections

### ğŸ“‰ Gradient Descent on Polynomial
- Define a 4th-order non-convex polynomial
- Implement gradient descent manually
- Track convergence to local minima
- Plot value evolution over iterations

### ğŸ” Backpropagation
- Train a neural net from scratch on 2D data
- Implement forward and backward passes
- Derive gradients using chain rule
- Use sigmoid activations + MSE loss
- Visualize loss over training epochs

### âš™ï¸ Optimization & Training Insights
- Explain optimizers: SGD, Adam, Adagrad with intuition
- Discuss model calibration: what it means, how to improve
- Address fairness in models and common shortcut learning issues
- Share best practices for robust ML training

---

## ğŸ› ï¸ Tools & Environment

- **Platform:** Kaggle Notebooks  
- **Languages:** Python (NumPy, Matplotlib, Scikit-learn)  
- **No frameworks:** All core logic implemented without TensorFlow, PyTorch, or high-level libraries

---

## ğŸ“¬ Contact

Sudhin Karki

I'm actively sharing projects, insights, and tutorials on AI engineering.

- ğŸ“˜ [LinkedIn](https://www.linkedin.com/in/sudhin-karki-bbb572237/)
- ğŸ¦ [Twitter](https://x.com/SudhinKarki)

Feel free to connect or drop a message if you'd like to collaborate or discuss these implementations.


These tasks form part of my AI engineering application portfolio. Feel free to reach out if you'd like to discuss the implementations or training insights further.


